{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c46ccd5b",
   "metadata": {},
   "source": [
    "# Feature Engineering: Repetition and Phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e574cf55",
   "metadata": {},
   "source": [
    "I've found two good ways to get ngrams:\n",
    "\n",
    "   1. Using Gensim's [Phrases model](https://radimrehurek.com/gensim_3.8.3/models/phrases.html) iteratively across the corpus, where the kth iteration creates a kgram\n",
    "   2. Using one of SKLearn's text feature extraction modules [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) or [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer), which is equivalent to the CountVectorizer followed by the [TfidfTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8618370d",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0b7377",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize, regexp_tokenize\n",
    "import gensim\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models import Phrases\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9500b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df70d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../data/metascripts/metascript_df_profanity', 'rb') as file:\n",
    "    metascripts = pickle.load(metascripts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2c8de0",
   "metadata": {},
   "source": [
    "## prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40578d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = list(metascripts['description'].values())\n",
    "scripts = list(metascripts['transcript'].values())\n",
    "scripts_dict = zip(descriptions, scripts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f408c57",
   "metadata": {},
   "source": [
    "## with gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db04fb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_scripts = [regexp_tokenize(transcript, r\"['\\-\\w]+\") for description, transcript in scripts_dict.items()]\n",
    "docs_lem = [[lemmatizer.lemmatize(tok.lower()) for tok in transcript] for transcript in tok_scripts]\n",
    "docs_no_lem = [[tok.lower() for tok in transcript] for transcript in tok_scripts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e282367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# still not picking up anything greater than a bigram. May need to reduce the Phrases threshold.\n",
    "\n",
    "def append_ngrams(docs, ngram):\n",
    "    for idx in range(len(docs)):\n",
    "        for token in ngram[docs[idx]]:\n",
    "            if '_' in token:\n",
    "                # if token is an ngram, add to document.\n",
    "                docs[idx].append(token)\n",
    "    return docs\n",
    "\n",
    "def make_ngrams(tok_corpus, with_dict = False, lemmatize = True, max_n = 2, min_count = 5, **kwargs):\n",
    "    ngram_dict = {}\n",
    "    if lemmatize:\n",
    "        docs = [[lemmatizer.lemmatize(tok.lower()) for tok in transcript] for transcript in tok_corpus]\n",
    "    else:\n",
    "        docs = [[tok.lower() for tok in transcript] for transcript in tok_corpus]\n",
    "    for n in range(2, max_n+1):\n",
    "        if n == 2:\n",
    "            ngram_dict[f'{str(n)}grams'] = Phrases(docs, min_count = min_count, **kwargs)\n",
    "        else:\n",
    "            ngram_dict[f'{str(n)}grams'] = Phrases(ngram_dict[f'{str(n-1)}grams'][docs], min_count = min_count, **kwargs)\n",
    "    docs = append_ngrams(docs, ngram_dict[f'{str(max_n)}grams'])\n",
    "    if with_dict:\n",
    "        return docs, ngram_dict\n",
    "    else:\n",
    "        return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abb4079",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs, ngram_dict = make_ngrams(tok_scripts, with_dict = True, lemmatize = True, max_n = 4, min_count = 1, threshold = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dc6a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d01377b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind in range(len(docs)):\n",
    "    c = Counter(tok for tok in docs[ind] if re.search(\"(.+_){2}\", tok))\n",
    "    if len(c) > 0:\n",
    "        print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fc6e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "[(descriptions[ind], len(re.findall(\"what is that\", script.lower()))) for ind, script in enumerate(scripts) if len(re.findall(\"what is that\", script.lower())) > 0 ][15:19]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b763021",
   "metadata": {},
   "source": [
    "## with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c252068",
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_vectorizer = CountVectorizer(lowercase = True, \n",
    "                             token_pattern = r\"\\b[a-zA-z][a-zA-Z\\-']*\\b\", \n",
    "                             ngram_range = (1, 7),\n",
    "                             stop_words = \"english\",\n",
    "                             \n",
    "                            )\n",
    "scripts_tf = ct_vectorizer.fit_transform(scripts)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(**ct_vectorizer.get_params())\n",
    "scripts_tfidf = tfidf_vectorizer.fit_transform(scripts)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e34a5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer.get_feature_names_out()[8000:8015]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8560ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "scripts_tf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
