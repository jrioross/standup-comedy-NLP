{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69e8d4ec",
   "metadata": {},
   "source": [
    "# Feature Engineering: Words & Sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8ed747",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43076759",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize, regexp_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import gensim\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models import Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e147d2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07eb9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts_df = pd.read_pickle('../data/transcripts_raw_df.pickle')\n",
    "\n",
    "with open(f'../data/imdb_title_results_2022-05-23.pickle', 'rb') as file:\n",
    "    show_meta = pickle.load(file)\n",
    "    \n",
    "with open(f'../data/metascripts_df_2022-05-28.pickle', 'rb') as file:\n",
    "    metascripts = pickle.load(file)\n",
    "    \n",
    "with open(\"../data/profane_to_replace.pickle\", 'rb') as file:\n",
    "    to_replace = pickle.load(file)\n",
    "    \n",
    "with open(\"../data/profane_replace_with.pickle\", 'rb') as file:\n",
    "    replace_with = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd49cca8",
   "metadata": {},
   "source": [
    "## prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff72cb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nonenglish_shows = ['BILL BURR: WHY DO I DO THIS (2008) – Testo italiano completo',\n",
    "                   'DOUG STANHOPE: NO REFUNDS (2007) – Trascrizione italiana',\n",
    "                   'GEORGE CARLIN: JAMMING IN NEW YORK (1992) – Testo italiano completo',\n",
    "                   'GEORGE CARLIN: YOU ARE ALL DISEASED (1999) – Testo italiano completo',\n",
    "                   'GEORGE CARLIN: IT’S BAD FOR YA! (2008) – Testo italiano completo',\n",
    "                   'DAVE CHAPPELLE: THE BIRD REVELATION (2017) – Transcripción completa']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe34181b",
   "metadata": {},
   "outputs": [],
   "source": [
    "metascripts = metascripts[~metascripts['description'].isin(nonenglish_shows)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9861c825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace bracket and parenthetical content from scripts\n",
    "# Replace censored profanity with actual profanity\n",
    "metascripts['transcript'] = (metascripts['transcript']\n",
    "                                 .replace(\"\\[.+?\\]|\\(.+?\\)\",\"\", regex = True)\n",
    "                                 .replace(\"\\’|\\‘\", \"'\", regex = True)\n",
    "                                 .replace(\"\\“|\\”\", '\"', regex = True)\n",
    "                                 .replace(to_replace[0], replace_with[0], regex = True)\n",
    "                                 .replace(to_replace[1], replace_with[1], regex = True)\n",
    "                            )\n",
    "\n",
    "# Fill censored words to clean up our profanity detection\n",
    "profanity_fill = json.load(open('../data/profanity_fill.json'))\n",
    "\n",
    "for key, value in profanity_fill.items(): \n",
    "    metascripts['transcript'] = metascripts['transcript'].str.replace(key, value, regex = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ef55b0",
   "metadata": {},
   "source": [
    "## prepare lists and dictionaries for streamlined work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbbbcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts_dict = dict(zip(metascripts['description'].values, metascripts['transcript'].values))\n",
    "descriptions = list(transcripts_dict.keys())\n",
    "scripts = list(transcripts_dict.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf43b9d5",
   "metadata": {},
   "source": [
    "## word lengths\n",
    "word lengths are calculated as letters per word\n",
    "\n",
    "   * tokenize words (allow apostrophes and dashes but not numbers)\n",
    "   * do not lemmatize\n",
    "   * do not remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6794f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_cased = [regexp_tokenize(transcript, r\"[a-zA-Z]+\") for description, transcript in transcripts_dict.items()]\n",
    "bow_counter = [Counter(word.lower() for word in script_words) for script_words in bow_cased]\n",
    "\n",
    "tokenized_list = [[word.lower() for word in script_words] for script_words in bow_cased]\n",
    "dictionary = Dictionary(tokenized_list)\n",
    "corpus = [dictionary.doc2bow(script) for script in tokenized_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d712307f",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_lengths = [[len(word) for word in script_words] for script_words in tokenized_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2911acbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "metascripts['mean word length'] = [np.mean(script_word_lengths) for script_word_lengths in word_lengths]\n",
    "metascripts['std word length'] = [np.std(script_word_lengths) for script_word_lengths in word_lengths]\n",
    "\n",
    "for quantile in (0.25, 0.50, 0.75):\n",
    "    metascripts[f'Q{quantile/0.25} word length'] = [np.quantile(script_word_lengths, quantile) for script_word_lengths in word_lengths]\n",
    "\n",
    "metascripts['max word length'] = [np.max(script_word_lengths) for script_word_lengths in word_lengths]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893570fc",
   "metadata": {
    "hideCode": true
   },
   "source": [
    "## sentence lengths\n",
    "sentence lengths are calculated as words per sentence\n",
    "\n",
    "   * tokenize sentences and then count whitespaces\n",
    "   * do not remove stopwords\n",
    "   * get arrays so we can do mean, median, boxplot values, standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0daef686",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokenized_list = [sent_tokenize(transcript) for description, transcript in transcripts_dict.items()]\n",
    "sent_words_tokenized_list = [[regexp_tokenize(sent, r\"['\\-\\w]+\") for sent in sent_script] for sent_script in sent_tokenized_list]\n",
    "sent_lengths = [[len(sent) for sent in script] for script in sent_words_tokenized_list]\n",
    "sent_counts = [len(script) for script in sent_tokenized_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7913c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "metascripts['mean sentence length'] = [np.mean(script_sent_lengths) for script_sent_lengths in sent_lengths]\n",
    "metascripts['std sentence length'] = [np.std(script_sent_lengths) for script_sent_lengths in sent_lengths]\n",
    "\n",
    "for quantile in (0.25, 0.50, 0.75):\n",
    "    metascripts[f'Q{quantile/0.25} sentence length'] = [np.quantile(script_sent_lengths, quantile) for script_sent_lengths in sent_lengths]\n",
    "\n",
    "metascripts['max sentence length'] = [np.max(script_sent_lengths) for script_sent_lengths in sent_lengths]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0ec18c",
   "metadata": {},
   "source": [
    "## distinct words\n",
    "count distinct words in each show and normalize by determining the proportion of distinct words and distinct words per sentence\n",
    "\n",
    "   * tokenize: allow apostrophes and dashes but not numbers \n",
    "   * lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851134cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ab2843",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lem_counter = [Counter(lemmatizer.lemmatize(word.lower()) for word in script_words) for script_words in bow_cased]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6978ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_word_counts = [len(script_lem_counts) for script_lem_counts in lem_counter]\n",
    "total_word_counts = [np.sum([count for lem, count in script_lem_counts.items()]) for script_lem_counts in lem_counter]\n",
    "unique_total_ratio = [unique/total for unique, total in zip(unique_word_counts, total_word_counts)]\n",
    "unique_per_sent = [unique/sent_count for unique, sent_count in zip(unique_word_counts, sent_counts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a90de55",
   "metadata": {},
   "outputs": [],
   "source": [
    "metascripts['unique words'] = unique_word_counts\n",
    "metascripts['total words'] = total_word_counts\n",
    "metascripts['proportion unique words'] = unique_total_ratio\n",
    "metascripts['unique words per sentence'] = unique_per_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcd4f64",
   "metadata": {},
   "source": [
    "## words per minute and sentences per minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40872eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tok_scripts = [regexp_tokenize(script, r\"[\\w'-]+\") for script in scripts]\n",
    "words_per_minute = [len(script_words)/minutes for script_words, minutes in zip(word_tok_scripts, metascripts['runtimeMins'].values)]\n",
    "\n",
    "sent_tok_scripts = [sent_tokenize(script) for script in scripts]\n",
    "sent_per_minute = [len(script_sentences)/minutes for script_sentences, minutes in zip(sent_tok_scripts, metascripts['runtimeMins'].values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3986487",
   "metadata": {},
   "outputs": [],
   "source": [
    "metascripts['words per minute'] = words_per_minute\n",
    "metascripts['sentences per minute'] = sent_per_minute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643c4acb",
   "metadata": {},
   "source": [
    "## pickle updated metascripts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cd2f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../data/metascripts/metascript_df_ws.pickle', 'wb') as file:\n",
    "    pickle.dump(metascripts, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
