{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering for Standup Scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "import itertools\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize, regexp_tokenize\n",
    "import gensim\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts_df = pd.read_pickle('../data/transcripts_raw_df.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../data/imdb_title_results_2022-05-23.pickle', 'rb') as file:\n",
    "    show_meta = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word lengths, sentence lengths, distinct words\n",
    "- word lengths (letters per word)\n",
    "    * tokenize words (allow apostrophes and dashes but not numbers)\n",
    "    * do not lemmatize\n",
    "    * do not remove stopwords\n",
    "- sentence lengths (words per sentence)\n",
    "    * tokenize sentences and then count whitespaces\n",
    "    * do not remove stopwords\n",
    "    * get arrays so we can do mean, median, boxplot values, standard deviation\n",
    "- distinct words per total words\n",
    "    * tokenize and lemmatize words (allow apostrophes and dashes by not numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = transcripts_df['description'].str.split(':')\n",
    "weird_labels = [(index, split) for index, split in enumerate(splits) if len(split) == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts_dict = dict(zip(transcripts_df['description'].values, transcripts_df['transcript'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = list(transcripts_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_cased = [regexp_tokenize(transcript, r\"[a-zA-Z]+\") for description, transcript in transcripts_dict.items()]\n",
    "bow_counter = [Counter(word.lower() for word in script_words) for script_words in bow_cased]\n",
    "\n",
    "tokenized_list = [[word.lower() for word in script_words] for script_words in bow_cased]\n",
    "dictionary = Dictionary(tokenized_list)\n",
    "corpus = [dictionary.doc2bow(script) for script in tokenized_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_lengths = [[len(word) for word in script_words] for script_words in tokenized_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts_df['mean word length'] = [np.mean(script_word_lengths) for script_word_lengths in word_lengths]\n",
    "transcripts_df['std word length'] = [np.std(script_word_lengths) for script_word_lengths in word_lengths]\n",
    "\n",
    "for quantile in (0.25, 0.50, 0.75):\n",
    "    transcripts_df[f'Q{quantile/0.25} word length'] = [np.quantile(script_word_lengths, quantile) for script_word_lengths in word_lengths]\n",
    "\n",
    "transcripts_df['max word length'] = [np.max(script_word_lengths) for script_word_lengths in word_lengths]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sentence lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokenized_list = [sent_tokenize(transcript) for description, transcript in transcripts_dict.items()]\n",
    "sent_words_tokenized_list = [[regexp_tokenize(sent, r\"[’'\\-\\w]+\") for sent in sent_script] for sent_script in sent_tokenized_list]\n",
    "sent_lengths = [[len(sent) for sent in script] for script in sent_words_tokenized_list]\n",
    "sent_counts = [len(script) for script in sent_tokenized_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts_df['mean sentence length'] = [np.mean(script_sent_lengths) for script_sent_lengths in sent_lengths]\n",
    "transcripts_df['std sentence length'] = [np.std(script_sent_lengths) for script_sent_lengths in sent_lengths]\n",
    "\n",
    "for quantile in (0.25, 0.50, 0.75):\n",
    "    transcripts_df[f'Q{quantile/0.25} sentence length'] = [np.quantile(script_sent_lengths, quantile) for script_sent_lengths in sent_lengths]\n",
    "\n",
    "transcripts_df['max sentence length'] = [np.max(script_sent_lengths) for script_sent_lengths in sent_lengths]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## distinct words and distinct words per total words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lem_counter = [Counter(lemmatizer.lemmatize(word.lower()) for word in script_words) for script_words in bow_cased]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_word_counts = [len(script_lem_counts) for script_lem_counts in lem_counter]\n",
    "total_word_counts = [np.sum([count for lem, count in script_lem_counts.items()]) for script_lem_counts in lem_counter]\n",
    "unique_total_ratio = [unique/total for unique, total in zip(unique_word_counts, total_word_counts)]\n",
    "unique_per_sent = [unique/sent_count for unique, sent_count in zip(unique_word_counts, sent_counts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts_df['unique words'] = unique_word_counts\n",
    "transcripts_df['total words'] = total_word_counts\n",
    "transcripts_df['proportion unique words'] = unique_total_ratio\n",
    "transcripts_df['unique words per sentence'] = unique_per_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## words per minute and sentences per minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## repetition and phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_scripts = [regexp_tokenize(transcript, r\"[’'\\-\\w]+\") for description, transcript in transcripts_dict.items()]\n",
    "docs = [[lemmatizer.lemmatize(tok.lower()) for tok in transcript] for transcript in tok_scripts]\n",
    "\n",
    "# Add bigrams and trigrams to docs (only ones that appear 5 times or more).\n",
    "ngram = Phrases(docs, min_count=5)\n",
    "for idx in range(len(docs)):\n",
    "    for token in ngram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            docs[idx].append(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## profanity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from profanityfilter import ProfanityFilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf = ProfanityFilter()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "542466b5a324f1333f512993cba41ecf153bff5c08c85763ac424cb7f020baf3"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
