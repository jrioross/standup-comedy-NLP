{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "173de068",
   "metadata": {},
   "source": [
    "# Feature Engineering: Repetition and Phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380bbda0",
   "metadata": {},
   "source": [
    "I've found two good ways to get ngrams:\n",
    "\n",
    "   1. Using Gensim's [Phrases model](https://radimrehurek.com/gensim_3.8.3/models/phrases.html) iteratively across the corpus, where the kth iteration creates a kgram\n",
    "   2. Using one of SKLearn's text feature extraction modules [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) or [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer), which is equivalent to the CountVectorizer followed by the [TfidfTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c959838b",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cb7479c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize, regexp_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import gensim\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models import Phrases\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d0caecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1edaa9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../data/metascripts_df_profanity.pickle', 'rb') as file:\n",
    "    metascripts = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b9b4ba",
   "metadata": {},
   "source": [
    "## prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85dc733f",
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = list(metascripts['description'].values)\n",
    "scripts = list(metascripts['transcript'].values)\n",
    "scripts_dict = dict(zip(descriptions, scripts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ceb44c",
   "metadata": {},
   "source": [
    "## use sklearn to get repetition of phrases up to 7grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ef31f88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get ngram term frequencies using count vectorizer\n",
    "ct_vectorizer = CountVectorizer(lowercase = True, \n",
    "                             token_pattern = r\"\\b[a-zA-z][a-zA-Z\\-']*\\b\", \n",
    "                             ngram_range = (1, 7),\n",
    "                             stop_words = \"english\"\n",
    "                            )\n",
    "scripts_tf = ct_vectorizer.fit_transform(scripts)\n",
    "\n",
    "# get ngram \"term frequency-inverse document frequncies\" using count vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(**ct_vectorizer.get_params())\n",
    "scripts_tfidf = tfidf_vectorizer.fit_transform(scripts)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "44e09c27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7159922"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tfidf_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3ced4dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine how many phrases are repeated and what proportion of phrases are repeated for each show\n",
    "threepeat_counts = (scripts_tf >= 3).sum(axis = 1).flatten().tolist()[0]\n",
    "threepeat_props = ((scripts_tf >= 3).sum(axis = 1) / (scripts_tf > 0).sum(axis = 1)).flatten().tolist()[0]\n",
    "\n",
    "# add columns to metascripts\n",
    "metascripts['threepeat counts'] = threepeat_counts\n",
    "metascripts['threepeat proportions'] = threepeat_props"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a1af4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "px.box(metascripts, x = 'threepeat proportions', hover_data = ['description'], points = 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bf2a84",
   "metadata": {},
   "source": [
    "## pickle results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5d025dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/metascripts_repetition_df.pickle', 'wb') as file:\n",
    "    pickle.dump(metascripts, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## appendix:\n",
    "### with gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_scripts = [regexp_tokenize(transcript, r\"['\\-\\w]+\") for description, transcript in scripts_dict.items()]\n",
    "docs_lem = [[lemmatizer.lemmatize(tok.lower()) for tok in transcript] for transcript in tok_scripts]\n",
    "docs_no_lem = [[tok.lower() for tok in transcript] for transcript in tok_scripts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# still not picking up anything greater than a bigram. May need to reduce the Phrases threshold.\n",
    "\n",
    "def append_ngrams(docs, ngram):\n",
    "    for idx in range(len(docs)):\n",
    "        for token in ngram[docs[idx]]:\n",
    "            if '_' in token:\n",
    "                # if token is an ngram, add to document.\n",
    "                docs[idx].append(token)\n",
    "    return docs\n",
    "\n",
    "def make_ngrams(tok_corpus, with_dict = False, lemmatize = True, max_n = 2, min_count = 5, **kwargs):\n",
    "    ngram_dict = {}\n",
    "    if lemmatize:\n",
    "        docs = [[lemmatizer.lemmatize(tok.lower()) for tok in transcript] for transcript in tok_corpus]\n",
    "    else:\n",
    "        docs = [[tok.lower() for tok in transcript] for transcript in tok_corpus]\n",
    "    for n in range(2, max_n+1):\n",
    "        if n == 2:\n",
    "            ngram_dict[f'{str(n)}grams'] = Phrases(docs, min_count = min_count, **kwargs)\n",
    "        else:\n",
    "            ngram_dict[f'{str(n)}grams'] = Phrases(ngram_dict[f'{str(n-1)}grams'][docs], min_count = min_count, **kwargs)\n",
    "    docs = append_ngrams(docs, ngram_dict[f'{str(max_n)}grams'])\n",
    "    if with_dict:\n",
    "        return docs, ngram_dict\n",
    "    else:\n",
    "        return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs, ngram_dict = make_ngrams(tok_scripts, with_dict = True, lemmatize = True, max_n = 4, min_count = 1, threshold = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind in range(len(docs)):\n",
    "    c = Counter(tok for tok in docs[ind] if re.search(\"(.+_){2}\", tok))\n",
    "    if len(c) > 0:\n",
    "        print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(descriptions[ind], len(re.findall(\"what is that\", script.lower()))) for ind, script in enumerate(scripts) if len(re.findall(\"what is that\", script.lower())) > 0 ][15:19]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
